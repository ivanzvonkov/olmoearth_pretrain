"""Post-process ingested Sentinel-2 data into the Helios dataset."""

import argparse
import csv
import multiprocessing
from datetime import timedelta

import numpy as np
import tqdm
from rslearn.data_sources import Item
from rslearn.dataset import Window
from rslearn.utils.mp import star_imap_unordered
from rslearn.utils.raster_format import GeotiffRasterFormat
from upath import UPath

from ..const import METADATA_COLUMNS

BANDS = [
    "B01",
    "B02",
    "B03",
    "B04",
    "B05",
    "B06",
    "B07",
    "B08",
    "B8A",
    "B09",
    "B10",
    "B11",
    "B12",
]


def convert_sentinel2(window_path: UPath, helios_path: UPath) -> None:
    """Add Sentinel-2 data for this window to the Helios dataset.

    Args:
        window_path: the rslearn window directory to read data from.
        helios_path: Helios dataset path to write to.
    """
    window = Window.load(window_path)
    layer_datas = window.load_layer_datas()
    raster_format = GeotiffRasterFormat()

    # Add frequent data.
    # We read the individual images and their timestamps, then write the stacked
    # images and CSV.
    layer_name = "sentinel2_freq"
    images = []
    timestamps = []
    for group_idx, group in enumerate(layer_datas[layer_name].serialized_item_groups):
        if len(group) != 1:
            raise ValueError(
                f"expected Sentinel-2 groups to have length 1 but got {len(group)}"
            )
        item = Item.deserialize(group[0])
        timestamp = item.geometry.time_range[0]
        raster_dir = window.get_raster_dir(layer_name, BANDS, group_idx)
        image = raster_format.decode_raster(raster_dir, window.bounds)
        images.append(image)
        timestamps.append(timestamp.isoformat())

    if len(images) > 0:
        stacked_image = np.concatenate(images, axis=0)
        raster_format.encode_raster(
            path=helios_path / "sentinel2_freq",
            projection=window.projection,
            bounds=window.bounds,
            array=stacked_image,
            fname=f"{window.name}.tif",
        )
        with (helios_path / "sentinel2_freq_meta" / f"{window.name}.csv").open(
            "w"
        ) as f:
            writer = csv.DictWriter(f, fieldnames=METADATA_COLUMNS)
            writer.writeheader()
            for group_idx, timestamp in enumerate(timestamps):
                writer.writerow(
                    dict(
                        example_id=window.name,
                        image_idx=group_idx,
                        start_time=timestamp,
                        end_time=timestamp,
                    )
                )

    # Add monthly data.
    # The monthly images are stored in different layers, so we read one image per
    # layer. Then we reconstruct the time range to match the dataset configuration. And
    # finally stack the images and write them along with CSV.
    images = []
    time_ranges = []
    for month_idx in range(1, 13):
        layer_name = f"sentinel2_mo{month_idx}"
        start_time = window.time_range[0] + timedelta(days=(month_idx - 7) * 30)
        end_time = start_time + timedelta(days=30)
        raster_dir = window.get_raster_dir(layer_name, BANDS)
        if not raster_dir.exists():
            continue
        image = raster_format.decode_raster(raster_dir, window.bounds)
        images.append(image)
        time_ranges.append((start_time.isoformat(), end_time.isoformat()))

    if len(images) > 0:
        stacked_image = np.concatenate(images, axis=0)
        raster_format.encode_raster(
            path=helios_path / "sentinel2_monthly",
            projection=window.projection,
            bounds=window.bounds,
            array=stacked_image,
            fname=f"{window.name}.tif",
        )
        with (helios_path / "sentinel2_monthly_meta" / f"{window.name}.csv").open(
            "w"
        ) as f:
            writer = csv.DictWriter(f, fieldnames=METADATA_COLUMNS)
            writer.writeheader()
            for image_idx, (start_time, end_time) in enumerate(time_ranges):
                writer.writerow(
                    dict(
                        example_id=window.name,
                        image_idx=image_idx,
                        start_time=start_time,
                        end_time=end_time,
                    )
                )


if __name__ == "__main__":
    multiprocessing.set_start_method("forkserver")

    parser = argparse.ArgumentParser(
        description="Post-process Helios data",
    )
    parser.add_argument(
        "--ds_path",
        type=str,
        help="Source rslearn dataset path",
        required=True,
    )
    parser.add_argument(
        "--helios_path",
        type=str,
        help="Destination Helios dataset path",
        required=True,
    )
    parser.add_argument(
        "--workers",
        type=int,
        help="Number of workers to use",
        default=32,
    )
    args = parser.parse_args()

    ds_path = UPath(args.ds_path)
    helios_path = UPath(args.helios_path)

    metadata_fnames = ds_path.glob("windows/*/*/metadata.json")
    jobs = []
    for metadata_fname in metadata_fnames:
        jobs.append(
            dict(
                window_path=metadata_fname.parent,
                helios_path=helios_path,
            )
        )

    p = multiprocessing.Pool(args.workers)
    outputs = star_imap_unordered(p, convert_sentinel2, jobs)
    for _ in tqdm.tqdm(outputs, total=len(jobs)):
        pass
    p.close()
