"""Code for configuring and running Helios experiments."""

import logging
import sys
from collections.abc import Callable
from dataclasses import dataclass
from typing import cast

import numpy as np
from olmo_core.config import Config, StrEnum
from olmo_core.distributed.utils import get_local_rank
from olmo_core.launch.beaker import BeakerLaunchConfig
from olmo_core.train import (
    TrainerConfig,
    prepare_training_environment,
    teardown_training_environment,
)
from olmo_core.train.callbacks import ConfigSaverCallback, WandBCallback
from olmo_core.utils import get_default_device, prepare_cli_environment, seed_all

from helios.data.constants import Modality
from helios.data.dataloader import HeliosDataLoaderConfig
from helios.data.dataset import HeliosDatasetConfig, collate_helios
from helios.data.normalize import Normalizer, Strategy
from helios.data.visualize import visualize_sample
from helios.nn.latent_mim import LatentMIMConfig
from helios.train.train_module.latent_mim import LatentMIMTrainModuleConfig
from helios.train.train_module.train_module import HeliosTrainModuleConfig

logger = logging.getLogger(__name__)

# TODO: Make this more agnostic to the training setup
# TODO: Add support for different model configs
# TODO: Add support for different train module configs


# maybe this build common components can be the same function for every experiment
@dataclass
class CommonComponents(Config):
    """Any configurable items that are common to all experiments."""

    run_name: str
    save_folder: str
    supported_modality_names: list[str]
    launch: BeakerLaunchConfig
    # callbacks: dict[str, Callback]

    def validate(self) -> None:
        """Validate the common components."""
        if not isinstance(self.supported_modality_names, list):
            raise ValueError("supported_modality_names must be a list")
        if not all(
            modality in Modality.names() for modality in self.supported_modality_names
        ):
            raise ValueError(
                "supported_modality_names must contain only valid modality names"
            )


@dataclass
class HeliosVisualizeConfig(Config):
    """Configuration for visualizing the dataset."""

    output_dir: str
    num_samples: int | None = None
    global_step: int | None = None
    normalize_strategy: Strategy = Strategy.PREDEFINED
    std_multiplier: float = 2.0


@dataclass
class HeliosExperimentConfig(Config):
    """Configuration for a Helios experiment."""

    run_name: str
    launch: BeakerLaunchConfig
    model: Config
    dataset: HeliosDatasetConfig  # will likely be fixed for us
    data_loader: HeliosDataLoaderConfig  # will likely be fixed for us
    train_module: HeliosTrainModuleConfig
    trainer: TrainerConfig
    visualize_config: HeliosVisualizeConfig | None = None
    init_seed: int = 12536


def split_common_overrides(overrides: list[str]) -> tuple[list[str], list[str]]:
    """Split the common overrides from the command line."""
    common_overrides = [
        dotfield.replace("common.", "")
        for dotfield in overrides
        if "common." in dotfield
    ]
    non_common_overrides = [
        dotfield for dotfield in overrides if "common." not in dotfield
    ]
    return common_overrides, non_common_overrides


def build_config(
    common: CommonComponents,
    model_config_builder: Callable[[CommonComponents], Config],
    dataset_config_builder: Callable[[CommonComponents], HeliosDatasetConfig],
    dataloader_config_builder: Callable[[CommonComponents], HeliosDataLoaderConfig],
    trainer_config_builder: Callable[[CommonComponents], TrainerConfig],
    train_module_config_builder: Callable[
        [CommonComponents],
        HeliosTrainModuleConfig,
    ],
    overrides: list[str],
    visualize_config_builder: (
        Callable[[CommonComponents], HeliosVisualizeConfig] | None
    ) = None,
) -> HeliosExperimentConfig:
    """Build a Helios experiment configuration."""
    # Overide common components
    common_overrides, overrides = split_common_overrides(overrides)
    logger.info("Common overrides: %s", common_overrides)
    common = common.merge(common_overrides)
    logger.info("Common: %s", common)
    model_config = model_config_builder(common)
    dataset_config = dataset_config_builder(common)
    dataloader_config = dataloader_config_builder(common)
    trainer_config = trainer_config_builder(common)
    train_module_config = train_module_config_builder(common)
    visualize_config = (
        visualize_config_builder(common) if visualize_config_builder else None
    )
    config = HeliosExperimentConfig(
        run_name=common.run_name,
        model=model_config,
        dataset=dataset_config,
        data_loader=dataloader_config,
        train_module=train_module_config,
        trainer=trainer_config,
        visualize_config=visualize_config,
        launch=common.launch,
    )
    logger.info("Overrides: %s", overrides)
    config = config.merge(overrides)
    return config


def train(config: HeliosExperimentConfig) -> None:
    """Train an experiment."""
    # Set RNG states on all devices. Also, done in prepare_training_environment
    seed_all(config.init_seed)

    # Build components.
    # TODO: Setup init device arg and allow the model to be inited on device of our choice rather than moved over
    model = config.model.build()
    device = get_default_device()
    model = model.to(device)
    train_module = config.train_module.build(model)
    dataset = config.dataset.build()
    # TODO: akward harcoding of the collator here
    data_loader = config.data_loader.build(
        dataset, collator=collate_helios, dp_process_group=train_module.dp_process_group
    )
    trainer = config.trainer.build(train_module, data_loader)

    # Record the config to W&B/Comet and each checkpoint dir.
    config_dict = config.as_config_dict()
    cast(WandBCallback, trainer.callbacks["wandb"]).config = config_dict
    cast(ConfigSaverCallback, trainer.callbacks["config_saver"]).config = config_dict

    trainer.fit()


def visualize(config: HeliosExperimentConfig) -> None:
    """Visualize the dataset for an experiment."""
    logger.info("Visualizing the dataset")
    if config.visualize_config is None:
        raise ValueError("visualize_config is not set")
    global_step = config.visualize_config.global_step
    dataset = config.dataset.build()
    if global_step is not None:
        data_loader = config.data_loader.build(
            dataset, collator=collate_helios, dp_process_group=None
        )
        sample_indices = data_loader.fast_forward(global_step)
    else:
        sample_indices = np.random.randint(
            0, len(dataset), config.visualize_config.num_samples
        )
    normalizer = Normalizer(
        strategy=config.visualize_config.normalize_strategy,
        std_multiplier=config.visualize_config.std_multiplier,
    )
    logger.info(f"sample indices: {sample_indices}")
    for sample_index in sample_indices:
        visualize_sample(
            dataset, sample_index, normalizer, config.visualize_config.output_dir
        )
    logger.info("Done visualizing the dataset")


def launch(config: HeliosExperimentConfig) -> None:
    """Launch an experiment."""
    logger.info("Launching the experiment")
    logger.info(config)
    # Set follow=False if you don't want to stream the logs to the terminal
    config.launch.launch(follow=True)


class SubCmd(StrEnum):
    """Subcommands for Helios experiments.

    modeled after olmo-core experiment.py potentially olmo-core might support this directly
    """

    launch = "launch"
    train = "train"
    train_single = "train_single"
    prep = "prep"
    launch_prep = "launch_prep"
    dry_run = "dry_run"
    visualize = "visualize"

    def prepare_environment(self) -> None:
        """Prepare the environment for the given subcommand."""
        if self in (
            SubCmd.launch,
            SubCmd.dry_run,
            SubCmd.prep,
            SubCmd.launch_prep,
            SubCmd.visualize,
        ):
            prepare_cli_environment()
        elif self == SubCmd.train:
            prepare_training_environment()
        elif self == SubCmd.train_single:
            prepare_training_environment(backend=None)
        else:
            raise NotImplementedError(self)

    def run(self, config: HeliosExperimentConfig) -> None:
        """Run the given subcommand."""
        if get_local_rank() == 0:
            print(config)
            # TODO: Add parameter count math to config
            # print(
            #     "\n"
            #     f"[b blue]Total parameters:[/]                {config.model.num_params:,d}\n"
            #     f"[b blue]Non-embedding parameters:[/]        {config.model.num_non_embedding_params:,d}"
            # )

        if self == SubCmd.launch:
            launch(config)
        elif self == SubCmd.dry_run:
            pass
        elif self == SubCmd.visualize:
            seed_all(config.init_seed)
            visualize(config)
        elif self == SubCmd.train:
            try:
                train(config)
            finally:
                teardown_training_environment()
        elif self == SubCmd.train_single:
            if config.train_module.dp_config is not None:
                logger.warning(
                    "'dp_config' is set to %s, but you can't use data parallelism when running on a single node. Disabling.",
                    config.train_module.dp_config,
                )
                config.train_module.dp_config = None
            try:
                train(config)
            finally:
                teardown_training_environment()
        elif self == SubCmd.prep:
            raise NotImplementedError
        elif self == SubCmd.launch_prep:
            raise NotImplementedError
        else:
            raise NotImplementedError(self)


def main(
    *,
    common_components_builder: Callable,
    model_config_builder: Callable[[CommonComponents], LatentMIMConfig],
    dataset_config_builder: Callable[[CommonComponents], HeliosDatasetConfig],
    dataloader_config_builder: Callable[[CommonComponents], HeliosDataLoaderConfig],
    trainer_config_builder: Callable[[CommonComponents], TrainerConfig],
    train_module_config_builder: Callable[
        [CommonComponents], LatentMIMTrainModuleConfig
    ],
    visualize_config_builder: (
        Callable[[CommonComponents], HeliosVisualizeConfig] | None
    ) = None,
) -> None:
    """Main entry point for Helios experiments.

    overrides:  A list of field attributes with dot notation, e.g. ``foo.bar=1``.

    """
    usage = f"""
[yellow]Usage:[/] [i blue]python[/] [i cyan]{sys.argv[0]}[/] [i b magenta]{'|'.join(SubCmd)}[/] [i b]RUN_NAME CLUSTER[/] [i][OVERRIDES...][/]
If running command on a local machine ie from a session, you can use the [b]local[/b] cluster name.
[b]Subcommands[/]
[b magenta]launch:[/]     Not Implemented. Launch the script on Beaker with the [b magenta]train[/] subcommand.
[b magenta]train:[/]       Run the trainer. You usually shouldn't invoke the script with this subcommand directly.
             Instead use [b magenta]launch[/] or run it with torchrun.
[b magenta]train_single:[/]       Run the trainer on a single device (GPU, CPU, MPS). num_nodes is ignored.
[b magenta]prep:[/]       Not Implemented. Prepare the dataset ahead of training to save GPU time.
[b magenta]launch_prep:[/] Not Implemented. Launch the script on Beaker with the [b magenta]prep[/] subcommand.
[b magenta]dry_run:[/]     Pretty print the config and exit.
[b magenta]visualize:[/]   Visualize the dataset.

[b]Examples[/]
    # Train on 4 GPUs across 2 nodes
    torchrun train.py train
    # Visualize the dataset
    python train.py visualize
    """.strip()

    if len(sys.argv) < 4 or sys.argv[1] not in set(SubCmd):
        import rich

        rich.get_console().print(usage, highlight=False)
        sys.exit(1)

    script, cmd, run_name, cluster, *overrides = sys.argv
    # TODO: we should probably have a single common components builder that can be used for all experiments
    common = common_components_builder(script, cmd, run_name, cluster, overrides)

    cmd = SubCmd(cmd)
    cmd.prepare_environment()
    config = build_config(
        common=common,
        model_config_builder=model_config_builder,
        dataset_config_builder=dataset_config_builder,
        dataloader_config_builder=dataloader_config_builder,
        trainer_config_builder=trainer_config_builder,
        train_module_config_builder=train_module_config_builder,
        visualize_config_builder=visualize_config_builder,
        overrides=overrides,
    )

    cmd.run(config)
