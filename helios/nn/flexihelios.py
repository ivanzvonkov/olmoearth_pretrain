"""Model code for the Helios model."""

import logging
import math
from dataclasses import dataclass
from enum import Enum
from typing import Any, NamedTuple

import torch
from einops import rearrange, repeat
from olmo_core.config import Config
from torch import Tensor, nn

from helios.data.constants import Modality, ModalitySpec
from helios.dataset.utils import get_modality_specs_from_names
from helios.nn.attention import Block
from helios.nn.encodings import (get_1d_sincos_pos_encoding,
                                 get_2d_sincos_pos_encoding_with_resolution,
                                 get_month_encoding_table)
from helios.nn.flexi_patch_embed import FlexiPatchEmbed
from helios.train.masking import MaskedHeliosSample, MaskValue

logger = logging.getLogger(__name__)


def get_modalities_to_process(
    available_modalities: list[str], supported_modality_names: list[str]
) -> list[str]:
    """Get the modalities to process."""
    modalities_to_process = set(supported_modality_names).intersection(
        set(available_modalities)
    )
    return list(modalities_to_process)


def return_modalities_from_dict(
    per_modality_input_tokens: dict[str, Tensor],
) -> list[str]:
    """Return the modalities from a dictionary of per modality input tokens."""
    return [
        key for key in per_modality_input_tokens.keys() if not key.endswith("_mask")
    ]


# Resolution of the input data in meters
BASE_GSD = 10


class PoolingType(str, Enum):
    """Strategy for pooling the tokens."""

    MAX = "max"
    MEAN = "mean"


class TokensAndMasks(NamedTuple):
    """Output to compute the loss on.

    Args:
        sentinel2: sentinel 2 data of shape (B, P_H, P_W, T, Band_Sets, D)
        sentinel2_mask: sentinel 2 mask indicating which tokens are masked/unmasked (B, P_H, P_W, T, Band_Sets)
        latlon: lat lon data containing geographical coordinates
        latlon_mask: lat lon mask indicating which coordinates are masked/unmasked
    """

    sentinel2_l2a: Tensor | None = None
    sentinel2_l2a_mask: Tensor | None = None
    sentinel1: Tensor | None = None
    sentinel1_mask: Tensor | None = None
    worldcover: Tensor | None = None
    worldcover_mask: Tensor | None = None
    latlon: Tensor | None = None
    latlon_mask: Tensor | None = None

    @property
    def device(self) -> torch.device:
        """Get the device of the tokens and masks."""
        if self.sentinel2_l2a is not None:
            return self.sentinel2_l2a.device
        else:
            # look for any other modality that is not None
            for modality in self._fields:
                if getattr(self, modality) is not None:
                    return getattr(self, modality).device
            raise ValueError("No data to get device from")

    # TODO: It seems like we want a lot of our named tuples to have this functionality so we should probably create a utility base class for the named tuples and double subclass
    @classmethod
    def get_masked_modality_name(cls, modality: str) -> str:
        """Get the masked modality name."""
        return f"{modality}_mask"

    def as_dict(self, return_none: bool = True) -> dict[str, Any]:
        """Convert the namedtuple to a dictionary.

        Returns:
            Dictionary representation of the namedtuple.
        """
        return_dict = {}
        for field in self._fields:
            val = getattr(self, field)
            if return_none:
                return_dict[field] = val
            else:
                if val is not None:
                    return_dict[field] = val
        return return_dict

    @property
    def modalities(self) -> list[str]:
        """Return all data fields."""
        return [x for x in self._fields if not x.endswith("mask")]

    def get_shape_dict(self) -> dict[str, tuple]:
        """Return a dictionary of the shapes of the fields."""
        return {x: getattr(self, x).shape for x in self._fields}

    @staticmethod
    def _flatten(x: Tensor) -> Tensor:
        return rearrange(x, "b ... d -> b (...) d")

    def flatten_tokens_and_masks(self) -> tuple[Tensor, Tensor]:
        """Return the flattened tokens and masks.

        Tokens will have shape [B, T, D] and masks will have shape [B, T]
        """
        flattened_x, flattened_masks = [], []
        for attr_name in self.modalities:
            mask_attr_name = self.get_masked_modality_name(attr_name)
            attr = getattr(self, attr_name)
            masked_attr = getattr(self, mask_attr_name)
            if attr is not None:
                if masked_attr is None:
                    raise ValueError(
                        f"Can't have present {attr_name} but None {mask_attr_name}"
                    )
                masked_attr = masked_attr.unsqueeze(dim=-1)
                flattened_x.append(self._flatten(attr))
                flattened_masks.append(self._flatten(masked_attr))

        x = torch.cat(flattened_x, dim=1)
        masks = torch.cat(flattened_masks, dim=1)[:, :, 0]
        return x, masks

    def pool_unmasked_tokens(
        self, pooling_type: PoolingType = PoolingType.MAX
    ) -> Tensor:
        """Pool the unmasked tokens.

        Args:
            pooling_type: Pooling type for the tokens
        """
        x, mask = self.flatten_tokens_and_masks()
        # 1s for online encoder, 0s elsewhere
        mask = (mask == MaskValue.ONLINE_ENCODER.value).long()
        x_for_pooling = x * mask.unsqueeze(-1)
        if pooling_type == PoolingType.MAX:
            x_for_pooling = x_for_pooling.masked_fill(
                ~mask.bool().unsqueeze(-1), -float("inf")
            )
            return x_for_pooling.max(dim=1).values
        elif pooling_type == PoolingType.MEAN:
            return x_for_pooling.sum(dim=1) / torch.sum(mask, -1, keepdim=True)
        else:
            raise ValueError(f"Invalid pooling type: {pooling_type}")


class FlexiHeliosPatchEmbeddings(nn.Module):
    """Module that patchifies and encodes the input data."""

    def __init__(
        self,
        supported_modality_names: list[str],
        max_patch_size: int,
        embedding_size: int,
    ):
        """Initialize the patch embeddings.

        Args:
            supported_modality_names: Which modalities from Modality this model
                instantiation supports
            max_patch_size: Maximum size of patches
            embedding_size: Size of embeddings
        """
        super().__init__()
        self.max_patch_size = max_patch_size
        self.embedding_size = embedding_size
        self.supported_modality_names = supported_modality_names
        # TODO: want to be able to remove certain bands and modalities
        self.per_modality_embeddings = nn.ModuleDict({})
        for modality in self.supported_modality_names:
            self.per_modality_embeddings[modality] = (
                self._get_patch_embedding_module_for_modality(modality)
            )

    @staticmethod
    def _get_embedding_module_name(modality: str, idx: int) -> str:
        """Get the embedding module name.

        Module Dicts require string keys
        """
        return f"{modality}__{idx}"

    def _get_patch_embedding_module_for_modality(self, modality: str) -> nn.Module:
        """Get the patch embedding module for a modality."""
        modality_spec = Modality.get(modality)
        # Based on the modality name we choose the way to embed the data

        # I likely will need to know about what the embedding strategy is in the forward as well
        # Static modality
        if modality_spec.get_tile_resolution() == 0:
            # static in space
            return nn.ModuleDict(
                {
                    self._get_embedding_module_name(modality, idx): nn.Linear(
                        len(channel_set_idxs), self.embedding_size
                    )
                    for idx, channel_set_idxs in enumerate(
                        modality_spec.bandsets_as_indices()
                    )
                }
            )
        else:
            return nn.ModuleDict(
                {
                    self._get_embedding_module_name(modality, idx): FlexiPatchEmbed(
                        in_chans=len(channel_set_idxs),
                        embedding_size=self.embedding_size,
                        patch_size=self.max_patch_size,
                    )
                    for idx, channel_set_idxs in enumerate(
                        modality_spec.bandsets_as_indices()
                    )
                }
            )

    def apply_embedding_to_modality(
        self, modality: str, input_data: MaskedHeliosSample, patch_size: int
    ) -> tuple[Tensor, Tensor]:
        """Apply embedding to a modality."""
        masked_modality_name = input_data.get_masked_modality_name(modality)
        modality_mask = getattr(input_data, masked_modality_name)
        modality_data = getattr(input_data, modality)

        modality_spec = Modality.get(modality)

        modality_tokens, modality_masks = [], []
        for idx, channel_set_indices in enumerate(modality_spec.bandsets_as_indices()):
            modality_specific_kwargs = {}
            # TODO: update to use the modlaity spec property here
            if not modality_spec.is_spatial:
                # static in time
                token_mask = modality_mask[..., idx]
            else:
                token_mask = modality_mask[:, 0::patch_size, 0::patch_size, ..., idx]
                modality_specific_kwargs = {"patch_size": patch_size}
            patchified_dims = token_mask.shape[1:]
            # Now apply the embedding to
            if self.is_any_data_seen_by_encoder(token_mask):
                patchified_data = modality_data[..., channel_set_indices]
                patchified_data = self.per_modality_embeddings[modality][
                    self._get_embedding_module_name(modality, idx)
                ](patchified_data, **modality_specific_kwargs)
            else:
                logger.info(f"modality {modality} is not seen by encoder")
                patchified_data = torch.empty(
                    modality_data.shape[0],
                    *patchified_dims,
                    self.embedding_size,
                    dtype=modality_data.dtype,
                    device=modality_data.device,
                )
            modality_tokens.append(patchified_data)
            modality_masks.append(token_mask)
        return torch.stack(modality_tokens, dim=-2), torch.stack(modality_masks, dim=-1)

    @staticmethod
    def is_any_data_seen_by_encoder(modality_mask: Tensor) -> bool:
        """Check if any data is seen by the encoder."""
        return modality_mask.min() == MaskValue.ONLINE_ENCODER.value

    def forward(
        self,
        input_data: MaskedHeliosSample,
        patch_size: int,
    ) -> dict[str, Tensor]:
        """Return flexibly patchified embeddings for each modality of the input data.

        Given a [B, H, W, (T), C] inputs, returns a [B, H, W, (T), b_s, D] output.

        We assume that the spatial masks are consistent for the given patch size,
        so that if patch_size == 2 then one possible mask would be
        [0, 0, 1, 1]
        [0, 0, 1, 1]
        [1, 1, 0, 0]
        [1, 1, 0, 0]
        for the H, W dimensions
        """
        output_dict = {}
        modalities_to_process = get_modalities_to_process(
            input_data.modalities, self.supported_modality_names
        )
        for modality in modalities_to_process:
            modality_tokens, modality_masks = self.apply_embedding_to_modality(
                modality, input_data, patch_size
            )
            output_dict[modality] = modality_tokens
            modality_mask_name = input_data.get_masked_modality_name(modality)
            output_dict[modality_mask_name] = modality_masks
        return output_dict


class FlexiHeliosCompositeEncodings(nn.Module):
    """Composite encodings for the FlexiHelios model."""

    def __init__(
        self,
        embedding_size: int,
        supported_modalities: list[ModalitySpec],
        max_sequence_length: int,
        use_channel_embs: bool = True,
        random_channel_embs: bool = False,
    ):
        """Initialize the composite encodings.

        Args:
            embedding_size: Size of token embeddings
            supported_modalities: Which modalities from Modality this model
                instantiation supports
            max_sequence_length: Maximum sequence length
            use_channel_embs: Whether to use learnable channel embeddings
            random_channel_embs: Initialize channel embeddings randomly (zeros if False)
        """
        super().__init__()
        self.embedding_size = embedding_size
        self.supported_modalities = supported_modalities
        self.supported_modality_names = [
            modality.name for modality in supported_modalities
        ]
        self.embedding_size = embedding_size
        self.max_sequence_length = (
            max_sequence_length  # This max sequence length is a time dim thing
        )
        # TODO: we need to be able to calculate the size of the param based on what types of embeddings it will get

        # we have 4 embeddings types (pos_in_time, pos_in_space, month, channel) so each get
        # 0.25 of the dimension
        self.embedding_dim_per_embedding_type = int(embedding_size * 0.25)
        # Position encodings for time dimension initialized to 1D sinusoidal encodings
        self.pos_embed = nn.Parameter(
            get_1d_sincos_pos_encoding(
                torch.arange(max_sequence_length),
                self.embedding_dim_per_embedding_type,
            ),
            requires_grad=False,
        )
        # M
        month_tab = get_month_encoding_table(self.embedding_dim_per_embedding_type)
        self.month_embed = nn.Embedding.from_pretrained(month_tab, freeze=True)
        if use_channel_embs:
            args = {"requires_grad": True}
        else:
            args = {"requires_grad": False}

        self.per_modality_channel_embeddings = nn.ParameterDict()
        for modality in self.supported_modalities:
            shape = (len(modality.band_sets), self.embedding_dim_per_embedding_type)
            if random_channel_embs:
                channel_embeddings = nn.Parameter(torch.rand(shape), **args)
            else:
                channel_embeddings = nn.Parameter(torch.zeros(shape), **args)
            self.per_modality_channel_embeddings[modality.name] = channel_embeddings

        self.apply(self._init_weights)

    def _init_weights(self, m: nn.Module) -> None:
        if isinstance(m, nn.Linear):
            # we use xavier_uniform following official JAX ViT:
            torch.nn.init.xavier_uniform_(m.weight)
            if isinstance(m, nn.Linear) and m.bias is not None:
                # TODO: fix the dtype here
                nn.init.constant_(m.bias, 0).to(torch.float32)

    @staticmethod
    def calculate_gsd_ratio(input_res: float, patch_size: int) -> float:
        """Calculate the Ground Sample Distance ratio."""
        return input_res * patch_size / BASE_GSD

    def _apply_encodings_per_modality(
        self,
        modality_name: str,
        modality_tokens: Tensor,
        timestamps: Tensor | None = None,
        patch_size: int | None = None,
        input_res: int | None = None,
    ) -> Tensor:
        """Apply the encodings to the patchified data based on modality type.

        Args:
            modality_name: Name of the modality being processed
            modality_tokens: Token embeddings for the modality
            timestamps: Optional timestamps for temporal encodings
            patch_size: Optional patch size for spatial encodings
            input_res: Optional input resolution for spatial encodings

        Returns:
            Tensor with encodings applied based on modality type
        """
        # TODO: Improve this implementation it is quite bad

        modality = Modality.get(modality_name)
        logger.debug(f"Applying encodings to modality {modality}")

        if modality_tokens.ndim == 3:
            # modality_tokens = [B, Band_Sets, D]; static in space, static in time
            b, b_s, _ = modality_tokens.shape
            ein_string, ein_dict = "b b_s d", {"b": b, "b_s": b_s}
        elif modality_tokens.ndim == 4:
            b, t, b_s, _ = modality_tokens.shape
            ein_string, ein_dict = "b t b_s d", {"b": b, "t": t, "b_s": b_s}
        elif modality_tokens.ndim == 5:
            b, h, w, b_s, _ = modality_tokens.shape
            ein_string, ein_dict = "b h w b_s d", {"b": b, "h": h, "w": w, "b_s": b_s}
        elif modality_tokens.ndim == 6:
            b, h, w, t, b_s, _ = modality_tokens.shape
            ein_string, ein_dict = (
                "b h w t b_s d",
                {"b": b, "h": h, "w": w, "t": t, "b_s": b_s},
            )
        else:
            raise ValueError(f"Unsupported tokens shape: {modality_tokens.shape}")

        device = modality_tokens.device
        modality_embed = torch.zeros(modality_tokens.shape, device=device)
        n = self.embedding_dim_per_embedding_type

        # Channel embeddings
        channel_embed = self.per_modality_channel_embeddings[modality.name]
        channel_embed = repeat(channel_embed, f"b_s d -> {ein_string}", **ein_dict).to(
            device
        )
        modality_embed[..., :n] += channel_embed

        if modality.is_multitemporal:
            # Time position encodings
            time_embed = repeat(self.pos_embed[:t], f"t d -> {ein_string}", **ein_dict)
            modality_embed[..., n : n * 2] += time_embed.to(device)

            # Month encodings
            assert timestamps is not None
            months = timestamps[:, :, 1]
            month_embed = self.month_embed(months)
            month_embed = repeat(month_embed, f"b t d -> {ein_string}", **ein_dict)
            modality_embed[..., n * 2 : n * 3] += month_embed.to(device)
        if modality.is_spatial:
            # Spatial encodings
            assert input_res is not None
            assert patch_size is not None
            gsd_ratio = self.calculate_gsd_ratio(input_res, patch_size)
            spatial_embed = get_2d_sincos_pos_encoding_with_resolution(
                grid_size=h,
                res=torch.ones(b, device=device) * gsd_ratio,
                encoding_dim=self.embedding_dim_per_embedding_type,
                device=device,
            )
            spatial_embed = rearrange(spatial_embed, "b (h w) d -> b h w d", h=h, w=w)
            spatial_embed = repeat(
                spatial_embed, f"b h w d -> {ein_string}", **ein_dict
            )
            modality_embed[..., n * 3 : n * 4] += spatial_embed
        return modality_tokens + modality_embed

    def forward(
        self,
        per_modality_input_tokens: dict[str, Tensor],
        timestamps: Tensor,
        patch_size: int,
        input_res: int = BASE_GSD,
    ) -> dict[str, Tensor]:
        """Apply the encodings to the patchified data.

        Args:
            per_modality_input_tokens: Tokens only for each modality
            timestamps: Timestamps of the data
            patch_size: Size of patches
            input_res: Resolution of the input data

        Returns:
            Tokens only for each modality
        """
        output_dict = {}
        available_modalities = return_modalities_from_dict(per_modality_input_tokens)
        modalities_to_process = get_modalities_to_process(
            available_modalities, self.supported_modality_names
        )
        for modality_name in modalities_to_process:
            output_dict[modality_name] = self._apply_encodings_per_modality(
                modality_name,
                per_modality_input_tokens[modality_name],
                timestamps=timestamps,
                patch_size=patch_size,
                input_res=input_res,
            )
        return output_dict


class FlexiHeliosBase(nn.Module):
    """FlexiHeliosBase is a base class for FlexiHelios models."""

    cross_attn: bool = False

    def __init__(
        self,
        embedding_size: int,
        max_sequence_length: int,
        use_channel_embs: bool,
        num_heads: int,
        mlp_ratio: float,
        depth: int,
        drop_path: float,
        supported_modalities: list[ModalitySpec],
        random_channel_embs: bool = False,
    ) -> None:
        """Initialize the FlexiHeliosBase class."""
        super().__init__()

        self.embedding_size = embedding_size
        self.supported_modalities = supported_modalities
        self.supported_modality_names = [x.name for x in supported_modalities]
        logger.info(f"modalities being used by model: {self.supported_modality_names}")

        self.max_sequence_length = max_sequence_length
        self.use_channel_embs = use_channel_embs
        self.random_channel_embs = random_channel_embs

        self.blocks = nn.ModuleList(
            [
                Block(
                    embedding_size,
                    num_heads,
                    mlp_ratio,
                    qkv_bias=True,
                    norm_layer=nn.LayerNorm,  # TODO: This should be configurable
                    cross_attn=self.cross_attn,
                    drop_path=drop_path,
                )
                for _ in range(depth)
            ]
        )

        self.composite_encodings = FlexiHeliosCompositeEncodings(
            embedding_size,
            self.supported_modalities,
            max_sequence_length,
            use_channel_embs,
            random_channel_embs,
        )
        self.apply(self._init_weights)

    def _init_weights(self, m: nn.Module) -> None:
        if isinstance(m, nn.Linear):
            # we use xavier_uniform following official JAX ViT:
            torch.nn.init.xavier_uniform_(m.weight)
            if isinstance(m, nn.Linear) and m.bias is not None:
                nn.init.constant_(m.bias, 0)

    @staticmethod
    def grab_modality_specific_dims(modality_data: Tensor) -> tuple[int, ...]:
        """Grab the modality specific dimensions from the modality data.

        Assumes [B, ..., C, D]

        Every modality will have a batch dimension, a channel dimension and embedding dimension.

        Args:
            modality_data: Modality data

        Returns:
            Modality specific dimensions
        """
        return modality_data.shape[1:-2] if modality_data.ndim > 3 else ()

    # is naming here confusing if one of these channels can be missing?
    def collapse_and_combine_hwtc(self, x: dict[str, Tensor]) -> tuple[Tensor, Tensor]:
        """Collapse the tokens and masks, respectively, into two tensors."""
        tokens, masks = [], []
        available_modalities = return_modalities_from_dict(x)
        modalities_to_process = get_modalities_to_process(
            available_modalities, self.supported_modality_names
        )
        for modality in modalities_to_process:
            masked_modality_name = MaskedHeliosSample.get_masked_modality_name(modality)
            x_modality = x[modality]
            x_modality_mask = x[masked_modality_name]
            tokens.append(rearrange(x_modality, "b ... d -> b (...) d"))
            masks.append(rearrange(x_modality_mask, "b ... -> b (...)"))
        tokens = torch.cat(tokens, dim=1)
        masks = torch.cat(masks, dim=1)

        return tokens, masks

    @staticmethod
    def _construct_einops_pattern(
        spatial_dims: tuple[int, ...],
    ) -> tuple[str, dict[str, int]]:
        """Given a tuple of spatial dimensions (e.g. [B, H, W, T, ...]).

        build (1) an einops rearrange pattern of the form:
            "d -> (dim0) (dim1) (dim2)... d"
        and (2) a dictionary mapping dim0..dimN to the actual sizes.

        This allows reshaping a single-dimensional tensor [D] into
        [B, H, W, T, ..., D] using einops.
        """
        dim_dict = {f"dim{i}": size for i, size in enumerate(spatial_dims)}
        # e.g., "d -> (dim0) (dim1) (dim2) (dim3) d"
        pattern_input = (
            "d -> " + " ".join(f"(dim{i})" for i in range(len(spatial_dims))) + " d"
        )
        return pattern_input, dim_dict

    def split_tokens_masks_and_dims(
        self, x: dict[str, Tensor]
    ) -> tuple[dict[str, Tensor], dict[str, Tensor], dict[str, tuple]]:
        """Split the tokens, masks, and dimensions out into separate dicts."""
        tokens_only_dict = {}
        original_masks_dict = {}
        modalities_to_dims_dict = {}
        # TODO: Should I have a dict like object that has methods that can return a mask or atoken here?
        available_modalities = return_modalities_from_dict(x)
        modalities_to_process = get_modalities_to_process(
            available_modalities, self.supported_modality_names
        )
        for modality in modalities_to_process:
            x_modality = x[modality]
            tokens_only_dict[modality] = x_modality
            modalities_to_dims_dict[modality] = x_modality.shape
            masked_modality_name = MaskedHeliosSample.get_masked_modality_name(modality)
            original_masks_dict[masked_modality_name] = x[masked_modality_name]
        return tokens_only_dict, original_masks_dict, modalities_to_dims_dict

    # GIVE more explicit function names
    @staticmethod
    def split_x_y(
        tokens: Tensor, mask: Tensor
    ) -> tuple[Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor]:
        """Splits tokens into three groups based on mask values.

        This function:
        1. Sorts tokens according to the mask and gathers them in order.
        2. Chooses tokens to be decoded (x) based on the mask value DECODER.
        3. Chooses tokens to be used as context (y) based on the mask value ONLINE_ENCODER.
        4. Identifies missing tokens (z) based on the mask value MISSING.
        5. Returns boolean masks for x, y, and z along with indices to revert to the original ordering.

        Args:
            tokens: Tokens to split of shape [B, T, D].
            mask: Mask of shape [B, T].

        Returns:
            x: Tokens to be decoded of shape [B, X_len, D].
            y: Tokens to be used as context of shape [B, Y_len, D].
            z: Missing tokens of shape [B, Z_len, D].
            x_mask: Binary mask for x tokens of shape [B, X_len].
            y_mask: Binary mask for y tokens of shape [B, Y_len]. 1 means the token is used in the attention.
            z_mask: Binary mask for z tokens of shape [B, Z_len].
            indices: Indices for restoring the original token ordering of shape [B, T].
        """
        # Check if we can use the optimized version for fixed token counts
        binarized_missing_mask = mask == MaskValue.MISSING.value
        missing_counts = binarized_missing_mask.sum(dim=1)

        # If no missing tokens and counts are consistent across batch
        if missing_counts.sum() == 0:
            binarized_decoder_mask = mask == MaskValue.DECODER.value
            binarized_online_encoder_mask = mask == MaskValue.ONLINE_ENCODER.value

            decoder_counts = binarized_decoder_mask.sum(dim=1)
            encoder_counts = binarized_online_encoder_mask.sum(dim=1)

            # Check if all batches have the same number of decoder and encoder tokens
            if (decoder_counts == decoder_counts[0]).all() and (
                encoder_counts == encoder_counts[0]
            ).all():
                return FlexiHeliosBase.split_x_y_no_missing_values_in_mask(tokens, mask)

        # Fall back to the hybrid approach if conditions aren't met
        org_mask_dtype = mask.dtype
        # Sort tokens by mask value (descending order)
        sorted_mask, indices = torch.sort(
            mask.int(), dim=1, descending=True, stable=True
        )
        logger.info(f"sorted_mask: {sorted_mask}")
        logger.info(f"indices: {indices}")
        tokens = tokens.gather(1, indices[:, :, None].expand_as(tokens))

        # Create binary masks for each category
        binarized_missing_mask = sorted_mask == MaskValue.MISSING.value
        binarized_target_encoder_only_mask = (
            sorted_mask == MaskValue.TARGET_ENCODER_ONLY.value
        )
        assert (
            binarized_target_encoder_only_mask.sum() == 0
        ), "TARGET_ENCODER_ONLY tokens should not be present these should be unmasked to encoder only value when we are in the target encoder"
        logger.info(
            f"binarized_target_encoder_only_mask: {binarized_target_encoder_only_mask.sum()}"
        )
        binarized_decoder_mask = sorted_mask == MaskValue.DECODER.value
        binarized_online_encoder_mask = sorted_mask == MaskValue.ONLINE_ENCODER.value

        # Calculate per-sample counts for each category
        missing_counts = binarized_missing_mask.sum(dim=1)  # [B]
        decoder_counts = binarized_decoder_mask.sum(dim=1)  # [B]
        encoder_counts = binarized_online_encoder_mask.sum(dim=1)  # [B]

        # Get maximum lengths for each category across the batch
        max_length_to_be_decoded = decoder_counts.max()
        max_length_of_unmasked_tokens = encoder_counts.max()
        max_length_of_missing_tokens = missing_counts.max()
        logger.info(f"max_length_to_be_decoded: {max_length_to_be_decoded}")
        logger.info(f"max_length_of_unmasked_tokens: {max_length_of_unmasked_tokens}")
        logger.info(f"max_length_of_missing_tokens: {max_length_of_missing_tokens}")
        # Create padded tensors for each category
        B, T, D = tokens.shape
        unmasked_tokens = torch.zeros(
            (B, max_length_of_unmasked_tokens, D),
            device=tokens.device,
            dtype=tokens.dtype,
        )
        tokens_to_decode = torch.zeros(
            (B, max_length_to_be_decoded, D),
            device=tokens.device,
            dtype=tokens.dtype,
        )
        missing_tokens = torch.zeros(
            (B, max_length_of_missing_tokens, D),
            device=tokens.device,
            dtype=tokens.dtype,
        )

        unmasked_tokens_mask = torch.zeros(
            (B, max_length_of_unmasked_tokens),
            device=tokens.device,
            dtype=org_mask_dtype,
        )
        tokens_to_decode_mask = torch.zeros(
            (B, max_length_to_be_decoded),
            device=tokens.device,
            dtype=org_mask_dtype,
        )
        missing_tokens_mask = torch.zeros(
            (B, max_length_of_missing_tokens),
            device=tokens.device,
            dtype=org_mask_dtype,
        )

        for b in range(B):
            logger.info(f"tokens: {tokens[b]}")
            logger.info(f"sorted_mask: {sorted_mask[b]}")
            # Get counts for this batch
            missing_count = missing_counts[b]
            decoder_count = decoder_counts[b]
            encoder_count = encoder_counts[b]
            logger.info(f"missing_count: {missing_count}")
            logger.info(f"decoder_count: {decoder_count}")
            logger.info(f"encoder_count: {encoder_count}")

            # Since we sorted in descending order, MISSING tokens come first
            if missing_count > 0:
                missing_tokens[b, :missing_count] = tokens[b, :missing_count]
                missing_tokens_mask[b, :missing_count] = 1
                logger.info(f"missing_tokens: {missing_tokens}")
                logger.info(f"missing_tokens_mask: {missing_tokens_mask}")

            decoder_start = missing_count
            logger.info(f"decoder_start: {decoder_start}")
            if decoder_count > 0:
                tokens_to_decode[b, :decoder_count] = tokens[
                    b, decoder_start : decoder_start + decoder_count
                ]
                tokens_to_decode_mask[b, :decoder_count] = 1
                logger.info(f"tokens_to_decode: {tokens_to_decode}")
                logger.info(f"tokens_to_decode_mask: {tokens_to_decode_mask}")
            encoder_start = decoder_start + decoder_count
            logger.info(f"encoder_start: {encoder_start}")
            # ONLINE_ENCODER tokens are at the end after sorting in descending order
            if encoder_count > 0:
                unmasked_tokens[b, :encoder_count] = tokens[
                    b, encoder_start : encoder_start + encoder_count
                ]
                unmasked_tokens_mask[b, :encoder_count] = 1
                logger.info(f"unmasked_tokens: {unmasked_tokens}")
                logger.info(f"unmasked_tokens_mask: {unmasked_tokens_mask}")

        return (
            unmasked_tokens,
            tokens_to_decode,
            missing_tokens,
            unmasked_tokens_mask,
            tokens_to_decode_mask,
            missing_tokens_mask,
            indices,
        )

    @staticmethod
    def split_x_y_no_missing_values_in_mask(
        tokens: Tensor, mask: Tensor
    ) -> tuple[Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor]:
        """Fully vectorized split_x_y for the case with fixed token counts and no missing tokens.

        This specialized version is used when:
        1. There are no missing tokens (MaskValue.MISSING) in any batch

        In this case, we can use direct slicing operations which are fully vectorized.

        Args:
            tokens: Tokens to split of shape [B, T, D].
            mask: Mask of shape [B, T].

        Returns:
            x: Tokens to be decoded of shape [B, X_len, D].
            y: Tokens to be used as context of shape [B, Y_len, D].
            z: Empty tensor of shape [B, 0, D] (no missing tokens).
            x_mask: Binary mask for x tokens of shape [B, X_len].
            y_mask: Binary mask for y tokens of shape [B, Y_len].
            z_mask: Empty tensor of shape [B, 0] (no missing tokens).
            indices: Indices for restoring the original token ordering of shape [B, T].
        """
        B, _, D = tokens.shape
        org_mask_dtype = mask.dtype
        # https://stackoverflow.com/a/68621610/2332296
        # move all non-masked values to the front of their rows
        # and all masked values to be decoded to the end of their rows
        sorted_mask, indices = torch.sort(
            mask.int(), dim=1, descending=True, stable=True
        )
        logger.info(f"sorted_mask: {sorted_mask}")
        logger.info(f"indices: {indices}")
        tokens = tokens.gather(1, indices[:, :, None].expand_as(tokens))
        binarized_decoder_mask = sorted_mask == MaskValue.DECODER.value
        binarized_online_encoder_mask = sorted_mask == MaskValue.ONLINE_ENCODER.value
        # cut off to the length of the longest sequence
        max_length_to_be_decoded = binarized_decoder_mask.sum(-1).max()
        max_length_of_unmasked_tokens = binarized_online_encoder_mask.sum(-1).max()
        # x will be the query tokens, and y will be the key / value tokens
        x = tokens[:, :max_length_to_be_decoded]
        y = tokens[:, -max_length_of_unmasked_tokens:]

        # the x_mask is just going to be used in the reconstruction, to know which
        # x tokens to add back into the token list. TODO is this even necessary? it could
        # get padded with noise tokens since we don't care about reconstruction at all
        # for a whole bunch of tokens
        x_mask = binarized_decoder_mask[:, :max_length_to_be_decoded].to(
            dtype=org_mask_dtype
        )
        # the y mask is going to be used to determine which of the y values take. True values
        # take part in the attention (we don't take the inverse here, unlike in the decoder)
        y_mask = binarized_online_encoder_mask[:, -max_length_of_unmasked_tokens:].to(
            dtype=org_mask_dtype
        )
        z = tokens.new_zeros((B, 0, D))  # Empty tensor
        z_mask = mask.new_zeros((B, 0))  # Empty tensor

        return x, y, z, x_mask, y_mask, z_mask, indices

    @staticmethod
    def combine_x_y(
        x: Tensor,
        y: Tensor,
        z: Tensor,
        x_mask: Tensor,
        y_mask: Tensor,
        z_mask: Tensor,
        indices: Tensor,
    ) -> Tensor:
        """Reintegrate the separated token sequences into their original order.

        This function combines:
        1. x tokens (to be decoded)
        2. y tokens (used as context)
        3. z tokens (missing tokens)

        The token masks zero out positions which are not used/needed,
        and the final scatter step re-applies the original ordering tracked in 'indices'.

        Args:
            x: Query tokens of shape [B, X_len, D].
            y: Key/value tokens of shape [B, Y_len, D].
            z: Missing tokens of shape [B, Z_len, D].
            x_mask: Binary mask for x tokens of shape [B, X_len].
            y_mask: Binary mask for y tokens of shape [B, Y_len].
            z_mask: Binary mask for z tokens of shape [B, Z_len].
            indices: Indices for restoring the original token ordering of shape [B, T].

        Returns:
            A merged tokens tensor of shape [B, T, D] with all tokens in their
            original positions.
        """
        # Get dimensions
        B, T = indices.shape[0], indices.shape[1]
        D = x.shape[-1]

        # Create empty tensor to hold all tokens
        tokens = torch.zeros((B, T, D), dtype=x.dtype, device=x.device)

        # Calculate counts for each category across all batches
        z_counts = z_mask.sum(dim=1).int()  # [B]
        x_counts = x_mask.sum(dim=1).int()  # [B]
        y_counts = y_mask.sum(dim=1).int()  # [B]

        # Create position indices for each token type
        # For z tokens (missing tokens)
        z_positions = (
            torch.arange(z.shape[1], device=z.device).unsqueeze(0).expand(B, -1)
        )
        z_valid = z_positions < z_counts.unsqueeze(1)

        # For x tokens (decoder tokens)
        x_positions = (
            torch.arange(x.shape[1], device=x.device).unsqueeze(0).expand(B, -1)
        )
        x_valid = x_positions < x_counts.unsqueeze(1)

        # For y tokens (encoder tokens)
        y_positions = (
            torch.arange(y.shape[1], device=y.device).unsqueeze(0).expand(B, -1)
        )
        y_valid = y_positions < y_counts.unsqueeze(1)

        # Calculate starting positions for each token type in the combined sequence
        z_start = torch.zeros(B, dtype=torch.long, device=z.device)
        x_start = z_counts
        y_start = T - y_counts

        # Create target indices for each token type
        z_target_indices = z_start.unsqueeze(1) + z_positions
        x_target_indices = x_start.unsqueeze(1) + x_positions
        y_target_indices = y_start.unsqueeze(1) + y_positions

        # Create batch indices for scatter operation
        batch_indices = torch.arange(B, device=z.device).unsqueeze(1)

        # Apply masks to tokens
        z_masked = z * z_mask.unsqueeze(-1)
        x_masked = x * x_mask.unsqueeze(-1)
        y_masked = y * y_mask.unsqueeze(-1)

        # Place z tokens (missing tokens)
        z_valid_flat = z_valid.flatten()
        if z_valid_flat.any():
            batch_indices_z = batch_indices.expand(-1, z.shape[1]).flatten()[
                z_valid_flat
            ]
            z_target_indices_flat = z_target_indices.flatten()[z_valid_flat]
            tokens[batch_indices_z, z_target_indices_flat] = z_masked.reshape(-1, D)[
                z_valid.reshape(-1)
            ]

        # Place x tokens (decoder tokens)
        x_valid_flat = x_valid.flatten()
        if x_valid_flat.any():
            batch_indices_x = batch_indices.expand(-1, x.shape[1]).flatten()[
                x_valid_flat
            ]
            x_target_indices_flat = x_target_indices.flatten()[x_valid_flat]
            tokens[batch_indices_x, x_target_indices_flat] = x_masked.reshape(-1, D)[
                x_valid.reshape(-1)
            ]

        # Place y tokens (encoder tokens)
        y_valid_flat = y_valid.flatten()
        if y_valid_flat.any():
            batch_indices_y = batch_indices.expand(-1, y.shape[1]).flatten()[
                y_valid_flat
            ]
            y_target_indices_flat = y_target_indices.flatten()[y_valid_flat]
            tokens[batch_indices_y, y_target_indices_flat] = y_masked.reshape(-1, D)[
                y_valid.reshape(-1)
            ]

        # Scatter tokens back to their original positions
        tokens = tokens.scatter(1, indices[:, :, None].expand_as(tokens), tokens)

        return tokens

    @staticmethod
    def split_and_expand_per_modality(
        x: Tensor, modalities_to_dims_dict: dict[str, tuple]
    ) -> dict[str, Tensor]:
        """Split and expand the tokens per modality.

        Args:
            x: Tokens to split and expand (b n d)
            modalities_to_dims_dict: Dictionary mapping modalities to their dimensions
        Returns:
            tokens_only_dict: mapping modalities to their tokens
        """
        tokens_only_dict = {}
        tokens_reshaped = 0
        for modality, dims in modalities_to_dims_dict.items():
            # Skip batch (first) and embedding (last) dimensions
            middle_dims = dims[1:-1]
            num_tokens_for_modality = math.prod(middle_dims)

            # Extract tokens for this modality (b n d)
            modality_tokens = x[
                :, tokens_reshaped : tokens_reshaped + num_tokens_for_modality
            ]

            # TODO: see if there  is a general and clean einops way to do this
            # Reshape to original dimensions (e.g., for 4D spatial dims: b d1 d2 d3 d4 e)
            x_modality = modality_tokens.view(x.shape[0], *middle_dims, x.shape[-1])

            tokens_reshaped += num_tokens_for_modality
            tokens_only_dict[modality] = x_modality

        return tokens_only_dict


class Encoder(FlexiHeliosBase):
    """Encoder module that processes masked input samples into token representations."""

    cross_attn: bool = False

    def __init__(
        self,
        embedding_size: int,
        max_patch_size: int,
        num_heads: int,
        mlp_ratio: float,
        depth: int,
        drop_path: float,
        supported_modalities: list[ModalitySpec],
        max_sequence_length: int,
        use_channel_embs: bool = True,
        random_channel_embs: bool = False,
    ):
        """Initialize the encoder.

        Args:
            embedding_size: Size of token embeddings
            max_patch_size: Maximum patch size for patchification
            num_heads: Number of attention heads
            mlp_ratio: Ratio for MLP hidden dimension
            depth: Number of transformer layers
            drop_path: Drop path rate
            supported_modalities: list documenting modalities used in a given model instantiation
            max_sequence_length: Maximum sequence length
            use_channel_embs: Whether to use learnable channel embeddings
            random_channel_embs: Initialize channel embeddings randomly (zeros if False)
        """
        super().__init__(
            embedding_size=embedding_size,
            depth=depth,
            mlp_ratio=mlp_ratio,
            num_heads=num_heads,
            max_sequence_length=max_sequence_length,
            use_channel_embs=use_channel_embs,
            drop_path=drop_path,
            supported_modalities=supported_modalities,
            random_channel_embs=random_channel_embs,
        )
        self.max_patch_size = max_patch_size
        self.embedding_size = embedding_size
        self.patch_embeddings = FlexiHeliosPatchEmbeddings(
            self.supported_modality_names,
            self.max_patch_size,
            self.embedding_size,
        )
        self.norm = nn.LayerNorm(self.embedding_size)
        self.apply(self._init_weights)

    def create_token_exit_ids(
        self, x: dict[str, Tensor], token_exit_cfg: dict[str, int]
    ) -> dict[str, Tensor]:
        """Create the token exit ids for # of layers of attention for each band group.

        Assumes modality channel groups are in the second to last dimension of the tokens.
        """
        exit_ids_per_modality_dict = {}
        available_modalities = return_modalities_from_dict(x)
        modalities_to_process = get_modalities_to_process(
            available_modalities, self.supported_modality_names
        )
        for modality in modalities_to_process:
            num_exit_layers = token_exit_cfg[modality]
            exit_seq_modality = torch.full_like(x[modality], fill_value=num_exit_layers)
            exit_ids_per_modality_dict[modality] = exit_seq_modality
        return exit_ids_per_modality_dict

    @staticmethod
    def remove_masked_tokens(x: Tensor, mask: Tensor) -> tuple[Tensor, Tensor, Tensor]:
        """Remove masked tokens from the tokens and masks.

        Implementation from https://stackoverflow.com/a/68621610/2332296

        On Input:
        1 means this token should be removed
        0 means this token should be kept

        Args:
            x: Tokens to remove masked tokens from
            mask: Mask to remove masked tokens from

        Returns:
            tokens: [B, T, D]
            indices: [B, T]
            updated_mask: [B, T]
            where T is the max number of unmasked tokens for an instance
        """
        org_mask_dtype = mask.dtype
        mask = mask.bool()
        # At this point when we flip the mask 1 means keep 0 means remove
        sorted_mask, indices = torch.sort(
            (~mask).int(), dim=1, descending=True, stable=True
        )
        # Now all the places where we want to keep the token are at the front of the tensor
        x = x.gather(1, indices[:, :, None].expand_as(x))
        # Now all tokens that should be kept are first in the tensor

        # set masked values to 0 (not really necessary since we'll ignore them anyway)
        x = x * sorted_mask.unsqueeze(-1)

        # cut off to the length of the longest sequence
        max_length = sorted_mask.sum(-1).max()
        x = x[:, :max_length]
        # New mask chopped to the longest sequence
        updated_mask = 1 - sorted_mask[:, :max_length]

        return x, indices, updated_mask.to(dtype=org_mask_dtype)

    @staticmethod
    def should_exit(i_blk: int, exit_after_n_layers: int | None) -> bool:
        """Determine if the current block should exit the attention layers."""
        if exit_after_n_layers is None:
            return False
        return i_blk >= exit_after_n_layers

    @staticmethod
    def add_removed_tokens(
        x: Tensor, indices: Tensor, mask: Tensor
    ) -> tuple[Tensor, Tensor]:
        """Add removed tokens to the tokens and masks.

        Args:
            x: Tokens to add removed tokens to
            indices: Original indices of the masked tokens
            mask: Mask to add removed tokens to

        Returns:
            tokens: Tokens with removed tokens added
            mask: Mask with removed tokens added
        """
        assert (
            x.shape[1] > 0
        ), "x must have at least one token we should not mask all tokens"
        masked_tokens = repeat(
            torch.zeros_like(x[0, 0, :]), "d -> b t d", b=x.shape[0], t=indices.shape[1]
        )
        full_mask = torch.cat(
            (
                mask,
                torch.ones(
                    (x.shape[0], indices.shape[1] - x.shape[1]),
                    device=x.device,
                    dtype=mask.dtype,
                ),
            ),
            dim=-1,
        )
        # can't set value on leaf variable
        out = masked_tokens.clone()
        # put tokens in full masked tensor (at the first N positions in every row)
        out[~full_mask.bool()] = x[~mask.bool()]
        # then move them to their original positions
        out = out.scatter(1, indices[:, :, None].expand_as(out), out)
        full_mask = full_mask.scatter(1, indices.expand_as(full_mask), full_mask)
        # Values that were masked out are not returned but the values that are still there are returned to the original positions
        return out, full_mask

    def create_exit_seqs(
        self,
        tokens_only_dict: dict[str, Tensor],
        mask_only_dict: dict[str, Tensor],
        token_exit_cfg: dict[str, int] | None,
    ) -> tuple[Tensor | None]:
        """Create the exit sequences and tokens."""
        # Check that tokens_only_dict doesn't contain any mask keys
        assert all(
            not key.endswith("_mask") for key in tokens_only_dict
        ), "tokens_only_dict should not contain mask keys"
        if token_exit_cfg:
            exit_ids_per_modality = self.create_token_exit_ids(
                tokens_only_dict, token_exit_cfg
            )
            exit_ids_per_modality.update(mask_only_dict)
            # Exit ids seqs tells us which layer to exit each token
            exit_ids_seq, _ = self.collapse_and_combine_hwtc(exit_ids_per_modality)
        else:
            exit_ids_seq = None
        return exit_ids_seq

    def apply_attn(
        self,
        x: dict[str, Tensor],
        timestamps: Tensor,
        patch_size: int,
        input_res: int,
        token_exit_cfg: dict[str, int] | None = None,
        exit_after_n_layers: int | None = None,
    ) -> dict[str, Tensor]:
        """Apply the attention to the tokens and masks."""
        tokens_only_dict, original_masks_dict, modalities_to_dims_dict = (
            self.split_tokens_masks_and_dims(x)
        )
        exit_ids_seq = self.create_exit_seqs(
            tokens_only_dict, original_masks_dict, token_exit_cfg
        )
        # exited tokens are just the linear projection
        exited_tokens, _ = self.collapse_and_combine_hwtc(x)

        tokens_dict = self.composite_encodings.forward(
            tokens_only_dict,
            timestamps,
            patch_size,
            input_res,
        )
        tokens_dict.update(original_masks_dict)
        x, mask = self.collapse_and_combine_hwtc(tokens_dict)

        new_mask = mask != MaskValue.ONLINE_ENCODER.value

        tokens, indices, new_mask = self.remove_masked_tokens(x, new_mask)
        if exit_ids_seq is not None:
            exit_ids_seq, _, _ = self.remove_masked_tokens(exit_ids_seq, mask)
            # still linear projections
            exited_tokens, _, _ = self.remove_masked_tokens(exited_tokens, mask)
        # Apply attn with varying encoder depths
        for i_blk, blk in enumerate(self.blocks):
            if self.should_exit(i_blk, exit_after_n_layers):
                # if exit_after is N, then we exit after the Nth layer
                # if exit_after is 0, then all layers are skipped
                break

            if exit_ids_seq is not None:
                # this should only ever be called by the target encoder,
                # in a torch.no_grad context
                assert exited_tokens is not None
                # If a token should exit, then we update the exit token with the current token at the same position
                exited_tokens = torch.where(
                    condition=(exit_ids_seq == i_blk),
                    input=tokens,
                    other=exited_tokens,
                )
            # we take the inverse of the mask because a value
            # of True indicates the value *should* take part in
            # attention
            # WARNING: THIS MAY CHANGE DEPENDING ON THE ATTENTION IMPLEMENTATION
            tokens = blk(x=tokens, y=None, attn_mask=~new_mask.bool())

        if exit_ids_seq is not None:
            # this should only ever be called by the target encoder,
            # in a torch.no_grad context
            assert exited_tokens is not None
            # full depth
            # IMPORTANT: write this to x
            tokens = torch.where(
                condition=(exit_ids_seq == (i_blk + 1)),  # 2 for full depth
                input=tokens,
                other=exited_tokens,
            )
        # we apply the norm before we add the removed tokens,
        # so that the norm is only computed against "real" tokens
        tokens = self.norm(tokens)
        # we don't care about the mask returned by add_removed_tokens, since we will
        # just use the original, unclipped mask here
        tokens, _ = self.add_removed_tokens(tokens, indices, new_mask)
        tokens_per_modality_dict = self.split_and_expand_per_modality(
            tokens, modalities_to_dims_dict
        )
        # merge original masks and the processed tokens
        tokens_per_modality_dict.update(original_masks_dict)
        return tokens_per_modality_dict

    def forward(
        self,
        x: MaskedHeliosSample,
        patch_size: int,
        input_res: int = BASE_GSD,
        exit_after_n_layers: int | None = None,
        token_exit_cfg: dict | None = None,
    ) -> TokensAndMasks:
        """Process masked input samples into token representations.

        Args:
            x: Masked input sample containing the data to be encoded
            patch_size: Size of patches to divide the input into
            input_res: Resolution of the input data
            exit_after_n_layers: Layer to exit after
            token_exit_cfg: Configuration for token exit

        Returns:
            TokensAndMasks containing the encoded representations and their masks
        """
        # TODO: Add step to validate the exit config is valid
        patchified_tokens_and_masks = self.patch_embeddings.forward(x, patch_size)
        if (exit_after_n_layers is None) or (exit_after_n_layers > 0):
            patchified_tokens_and_masks = self.apply_attn(
                x=patchified_tokens_and_masks,
                timestamps=x.timestamps,
                patch_size=patch_size,
                input_res=input_res,
                exit_after_n_layers=exit_after_n_layers,
                token_exit_cfg=token_exit_cfg,
            )
        return TokensAndMasks(**patchified_tokens_and_masks)


class Predictor(FlexiHeliosBase):
    """Predictor module that generates predictions from encoded tokens."""

    cross_attn = True

    def __init__(
        self,
        supported_modalities: list[ModalitySpec],
        encoder_embedding_size: int = 128,
        decoder_embedding_size: int = 128,
        depth: int = 2,
        mlp_ratio: float = 2.0,
        num_heads: int = 8,
        max_sequence_length: int = 24,
        drop_path: float = 0.0,
        learnable_channel_embeddings: bool = True,
        random_channel_embeddings: bool = False,
        output_embedding_size: int | None = None,
    ):
        """Initialize the predictor.

        Args:
            supported_modalities: modalities this model instantiation supports
            encoder_embedding_size: Size of encoder embeddings
            decoder_embedding_size: Size of decoder embeddings
            depth: Number of transformer layers
            mlp_ratio: Ratio for MLP hidden dimension
            num_heads: Number of attention heads
            max_sequence_length: Maximum sequence length
            drop_path: Drop path rate
            learnable_channel_embeddings: Whether to use learnable channel embeddings
            random_channel_embeddings: Whether to randomly initialize channel embeddings
            output_embedding_size: Size of output embeddings
        """
        super().__init__(
            embedding_size=decoder_embedding_size,
            depth=depth,
            mlp_ratio=mlp_ratio,
            num_heads=num_heads,
            max_sequence_length=max_sequence_length,
            drop_path=drop_path,
            use_channel_embs=learnable_channel_embeddings,
            random_channel_embs=random_channel_embeddings,
            supported_modalities=supported_modalities,
        )
        self.learnable_channel_embeddings = learnable_channel_embeddings
        self.random_channel_embeddings = random_channel_embeddings
        self.encoder_embedding_size = encoder_embedding_size
        self.encoder_to_decoder_embed = nn.Linear(
            encoder_embedding_size, decoder_embedding_size, bias=True
        )
        if output_embedding_size is None:
            output_embedding_size = encoder_embedding_size
        self.output_embedding_size = output_embedding_size
        self.to_output_embed = nn.Linear(
            decoder_embedding_size, output_embedding_size, bias=True
        )
        # THIS is the learnable mask token
        self.mask_token = nn.Parameter(torch.zeros(decoder_embedding_size))

        self.input_norm = nn.LayerNorm(encoder_embedding_size)
        self.norm = nn.LayerNorm(decoder_embedding_size)
        self.apply(self._init_weights)

    def add_masks(self, x: dict[str, Tensor]) -> dict[str, Tensor]:
        """Replace tokens that should be decoded (MaskValue.DECODER_ONLY) with the learnable mask token.

        in a dimension-agnostic way using einops. We assume the final dimension of each token tensor
        is the embedding dimension matching self.mask_token's size.
        """
        output_dict = {}
        available_modalities = return_modalities_from_dict(x)
        modalities_to_process = get_modalities_to_process(
            available_modalities, self.supported_modality_names
        )
        for modality in modalities_to_process:
            x_modality = x[modality]
            mask_name = MaskedHeliosSample.get_masked_modality_name(modality)

            # Check if the mask exists, if not, create a default mask with all zeros
            if mask_name in x:
                mask_modality = x[mask_name]
            else:
                # Create a default mask with all zeros (no tokens to be replaced)
                mask_modality = torch.zeros(
                    x_modality.shape[:-1],  # all dimensions except the last (embedding)
                    dtype=torch.float32,
                    device=x_modality.device,
                    requires_grad=x_modality.requires_grad,
                )

            # A boolean mask: True where tokens must be replaced by the mask token
            kept_mask = mask_modality == MaskValue.DECODER.value

            # Build the einops pattern and dimension dict
            spatial_dims = x_modality.shape[
                :-1
            ]  # all dimensions except the last (embedding)
            pattern_input, dim_dict = self._construct_einops_pattern(spatial_dims)

            mask_token_broadcasted = repeat(self.mask_token, pattern_input, **dim_dict)

            # Where kept_mask is True, use the broadcasted mask token
            x_modality = torch.where(
                kept_mask.unsqueeze(-1).bool(), mask_token_broadcasted, x_modality
            )

            output_dict[modality] = x_modality

        return output_dict

    def apply_attn(
        self,
        x: dict[str, Tensor],
        # THESE ARE NOT USED
        timestamps: Tensor,
        patch_size: int,
        input_res: int,
    ) -> dict[str, Tensor]:
        """Apply attention to the tokens."""
        tokens_only_dict, original_masks_dict, modalities_to_dims_dict = (
            self.split_tokens_masks_and_dims(x)
        )
        tokens_dict = self.add_masks(tokens_only_dict)
        tokens_dict.update(original_masks_dict)
        x, mask = self.collapse_and_combine_hwtc(tokens_dict)
        x, y, z, x_mask, y_mask, z_mask, indices = self.split_x_y(x, mask)
        for blk in self.blocks:
            # note that we are not taking the inverse of the mask, since split_x_y gives us
            # true values for values we want to take part in attention
            x = blk(x=x, y=y, attn_mask=y_mask.bool())
        x = self.combine_x_y(x, y, z, x_mask, y_mask, z_mask, indices)
        logger.info(f"x shape after combine_x_y {x.shape}")
        logger.info(f"x mask sum: {x_mask.sum()}")
        logger.info(f"y mask sum: {y_mask.sum()}")
        logger.info(f"z mask sum: {z_mask.sum()}")
        tokens_per_modality_dict = self.split_and_expand_per_modality(
            x, modalities_to_dims_dict
        )
        tokens_per_modality_dict.update(original_masks_dict)
        return tokens_per_modality_dict

    def is_any_data_to_be_decoded(self, modality_mask: Tensor) -> bool:
        """Check if any data is to be decoded for a given modality."""
        return modality_mask.max() == MaskValue.DECODER.value

    def forward(
        self,
        x: TokensAndMasks,
        timestamps: Tensor,
        patch_size: int,
        input_res: int = BASE_GSD,
    ) -> TokensAndMasks:
        """Generate predictions from encoded token representations.

        Args:
            x: TokensAndMasks containing the encoded tokens to make predictions from
            timestamps: Timestamps of the input data
            patch_size: Size of patches to divide the input into
            input_res: Resolution of the input data (in meters)

        Returns:
            TokensAndMasks containing the predicted tokens and their masks
        """
        decoder_emedded_dict = x._asdict()
        # Apply Input Norms and encoder to decoder embeds to each modality
        available_modalities = x.modalities
        modalities_to_process = get_modalities_to_process(
            available_modalities, self.supported_modality_names
        )
        for modality in modalities_to_process:
            x_modality = getattr(x, modality)
            x_modality = self.input_norm(x_modality)
            x_modality = self.encoder_to_decoder_embed(x_modality)
            masked_modality_name = x.get_masked_modality_name(modality)
            decoder_emedded_dict[modality] = x_modality
            decoder_emedded_dict[masked_modality_name] = getattr(
                x, masked_modality_name
            )

        tokens_only_dict = self.add_masks(decoder_emedded_dict)
        decoder_emedded_dict.update(tokens_only_dict)
        tokens_and_masks = self.apply_attn(
            decoder_emedded_dict, timestamps, patch_size, input_res
        )
        # TODO: Fac
        # tor this out into a more readable function
        output_dict = {}
        available_modalities = return_modalities_from_dict(tokens_and_masks)
        modalities_to_process = get_modalities_to_process(
            available_modalities, self.supported_modality_names
        )
        for modality in modalities_to_process:
            masked_modality_name = MaskedHeliosSample.get_masked_modality_name(modality)
            # Check if the mask exists, if not, create a default mask with all zeros
            if masked_modality_name in tokens_and_masks:
                modality_mask = tokens_and_masks[masked_modality_name]
            else:
                # Create a default mask with all zeros (no tokens to be replaced)
                modality_data = tokens_and_masks[modality]
                modality_mask = torch.zeros(
                    modality_data.shape[
                        :-1
                    ],  # all dimensions except the last (embedding)
                    dtype=torch.float32,  # should be configurable
                    device=modality_data.device,
                    requires_grad=modality_data.requires_grad,
                )

            # patchify masked data
            per_modality_output_tokens = []
            modality_data = tokens_and_masks[modality]
            modality_specific_dims = self.grab_modality_specific_dims(modality_data)

            band_sets = Modality.get(modality).band_sets
            for idx in range(len(band_sets)):
                if self.is_any_data_to_be_decoded(modality_mask):
                    logger.info(
                        f"modality_data in decoding {modality}: {modality_data.shape}"
                    )
                    per_channel_modality_data = modality_data[..., idx, :]
                    output_data = self.to_output_embed(
                        self.norm(per_channel_modality_data)
                    )
                else:
                    # If all data should be ignored by encoder, we need to return an empty tensor
                    output_data = torch.empty(
                        modality_data.shape[0],
                        *modality_specific_dims,
                        self.output_embedding_size,
                        dtype=modality_data.dtype,
                        device=modality_data.device,
                    )
                per_modality_output_tokens.append(output_data)
            output_dict[modality] = torch.stack(per_modality_output_tokens, dim=-2)
            output_dict[masked_modality_name] = modality_mask
        return TokensAndMasks(**output_dict)


@dataclass
class EncoderConfig(Config):
    """Configuration for the Encoder."""

    supported_modality_names: list[str]
    embedding_size: int = 16
    # This is the base patch size for the patch embedder
    max_patch_size: int = 8
    num_heads: int = 2
    mlp_ratio: float = 1.0
    depth: int = 2
    drop_path: float = 0.1
    max_sequence_length: int = 12
    use_channel_embs: bool = True
    random_channel_embs: bool = False

    def validate(self) -> None:
        """Validate the configuration."""
        if len(self.supported_modalities) == 0:
            raise ValueError("At least one modality must be added!")
        else:
            for modality in self.supported_modalities:
                if modality not in Modality.values():
                    raise ValueError(f"Modality {modality} is not supported")

    @property
    def supported_modalities(self) -> list[ModalitySpec]:
        """Get the supported modalities."""
        return get_modality_specs_from_names(self.supported_modality_names)

    def build(self) -> "Encoder":
        """Build the encoder."""
        self.validate()
        kwargs = self.as_dict(exclude_none=True, recurse=False)
        # supported_modality_names is replaced by supported_modalities
        kwargs.pop("supported_modality_names")
        kwargs["supported_modalities"] = self.supported_modalities
        logger.info(f"Encoder kwargs: {kwargs}")
        return Encoder(**kwargs)


@dataclass
class PredictorConfig(Config):
    """Configuration for the Predictor."""

    supported_modality_names: list[str]
    encoder_embedding_size: int = 16
    decoder_embedding_size: int = 16
    depth: int = 2
    mlp_ratio: float = 1.0
    num_heads: int = 2
    max_sequence_length: int = 12
    drop_path: float = 0.0
    learnable_channel_embeddings: bool = True
    random_channel_embeddings: bool = False
    output_embedding_size: int | None = None

    def validate(self) -> None:
        """Validate the configuration."""
        if len(self.supported_modalities) == 0:
            raise ValueError("At least one modality must be added!")
        else:
            for modality in self.supported_modalities:
                if modality not in Modality.values():
                    raise ValueError(f"Modality {modality} is not supported")

    @property
    def supported_modalities(self) -> list[ModalitySpec]:
        """Get the supported modalities."""
        return get_modality_specs_from_names(self.supported_modality_names)

    def build(self) -> "Predictor":
        """Build the predictor."""
        self.validate()
        kwargs = self.as_dict(exclude_none=True, recurse=False)
        # supported_modality_names is replaced by supported_modalities
        kwargs.pop("supported_modality_names")
        kwargs["supported_modalities"] = self.supported_modalities
        logger.info(f"Predictor kwargs: {kwargs}")
        return Predictor(**kwargs)

    # TODO: add multiple combo of variables for encoder and predictor, and being able to build them directly, no need to specify each parameter, e.g., encoder_tiny, encoder_small, encoder_base, encoder_large, etc.
    def build(self) -> "Predictor":
        """Build the predictor."""
        self.validate()
        kwargs = self.as_dict(exclude_none=True, recurse=False)
        # supported_modality_names is replaced by supported_modalities
        kwargs.pop("supported_modality_names")
        kwargs["supported_modalities"] = self.supported_modalities
        logger.info(f"Predictor kwargs: {kwargs}")
        return Predictor(**kwargs)

        # TODO: add multiple combo of variables for encoder and predictor, and being able to build them directly, no need to specify each parameter, e.g., encoder_tiny, encoder_small, encoder_base, encoder_large, etc.
        return Predictor(**kwargs)


# TODO: add multiple combo of variables for encoder and predictor, and being able to build them directly, no need to specify each parameter, e.g., encoder_tiny, encoder_small, encoder_base, encoder_large, etc.
